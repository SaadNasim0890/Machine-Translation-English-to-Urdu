{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13550772,
          "sourceType": "datasetVersion",
          "datasetId": 8606197
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/archive.zip -d /content/dataset\n",
        "!ls /content/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfeCFJnR3FI9",
        "outputId": "108dc891-1b3c-4ccf-eac9-e407d5b79447"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- Configuration ---\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20  # Start with a small number like 20-50\n",
        "LATENT_DIM = 256  # Dimensionality of the LSTM state\n",
        "NUM_SAMPLES = 20000  # Number of samples to train on (for speed)\n",
        "\n",
        "def create_dummy_files():\n",
        "    \"\"\"\n",
        "    Creates dummy corpus files for demonstration.\n",
        "    You can comment out or delete this function when using your real files.\n",
        "    \"\"\"\n",
        "    print(\"Creating dummy files (english-corpus.txt, urdu-corpus.txt)...\")\n",
        "\n",
        "    english_content = [\n",
        "        \"Hello world.\", \"How are you?\", \"This is a parallel corpus.\",\n",
        "        \"Python is a great language.\", \"I love machine learning.\", \"Run.\",\n",
        "        \"Where is the book?\", \"This is my house.\", \"What is your name?\",\n",
        "        \"See you later.\"\n",
        "    ]\n",
        "\n",
        "    urdu_content = [\n",
        "        \"ہیلو دنیا۔\", \"آپ کیسے ہیں؟\", \"یہ ایک متوازی کارپس ہے۔\",\n",
        "        \"پائتھون ایک بہترین زبان ہے۔\", \"مجھے مشین لرننگ سے محبت ہے۔\", \"بھاگو۔\",\n",
        "        \"کتاب کہاں ہے؟\", \"یہ میرا گھر ہے۔\", \"آپ کا نام کیا ہے؟\",\n",
        "        \"پھر ملیں گے.\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        with open(\"english-corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            for line in english_content: f.write(line + \"\\n\")\n",
        "        with open(\"urdu-corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            for line in urdu_content: f.write(line + \"\\n\")\n",
        "        print(\"Dummy files created successfully.\\n\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error creating dummy files: {e}\")\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Cleans and standardizes a single sentence.\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower().strip()\n",
        "    # Remove punctuation\n",
        "    sentence = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", sentence)\n",
        "    # Add start and end tokens\n",
        "    sentence = f\"[START] {sentence} [END]\"\n",
        "    return sentence\n",
        "\n",
        "def load_data(english_filepath, urdu_filepath, num_samples):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the data from text files.\n",
        "    \"\"\"\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    try:\n",
        "        with open(english_filepath, \"r\", encoding=\"utf-8\") as f_eng, \\\n",
        "             open(urdu_filepath, \"r\", encoding=\"utf-8\") as f_urd:\n",
        "\n",
        "            for eng_line, urd_line in zip(f_eng, f_urd):\n",
        "                if len(input_texts) >= num_samples:\n",
        "                    break\n",
        "\n",
        "                eng_line = eng_line.strip()\n",
        "                urd_line = urd_line.strip()\n",
        "\n",
        "                if eng_line and urd_line:\n",
        "                    input_texts.append(preprocess_sentence(eng_line))\n",
        "                    target_texts.append(preprocess_sentence(urd_line))\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please make sure both files exist.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loaded {len(input_texts)} sentence pairs.\")\n",
        "    return input_texts, target_texts\n",
        "\n",
        "def build_tokenizers(input_texts, target_texts):\n",
        "    \"\"\"\n",
        "    Creates and fits Keras Tokenizers for both languages.\n",
        "    \"\"\"\n",
        "    # Tokenizer for English (input)\n",
        "    input_tokenizer = Tokenizer(filters='')\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "\n",
        "    # Tokenizer for Urdu (target)\n",
        "    target_tokenizer = Tokenizer(filters='')\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "\n",
        "    return input_tokenizer, target_tokenizer\n",
        "\n",
        "def prepare_sequences(input_texts, target_texts, input_tokenizer, target_tokenizer):\n",
        "    \"\"\"\n",
        "    Converts text sentences to padded integer sequences.\n",
        "    \"\"\"\n",
        "    # Convert text to integer sequences\n",
        "    input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "    # Get max sequence lengths\n",
        "    max_encoder_seq_length = max(len(s) for s in input_sequences)\n",
        "    max_decoder_seq_length = max(len(s) for s in target_sequences)\n",
        "\n",
        "    print(f\"Max encoder sequence length: {max_encoder_seq_length}\")\n",
        "    print(f\"Max decoder sequence length: {max_decoder_seq_length}\")\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
        "    decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, max_encoder_seq_length, max_decoder_seq_length\n",
        "\n",
        "def build_model(input_vocab_size, target_vocab_size, max_encoder_seq_length, max_decoder_seq_length, latent_dim):\n",
        "    \"\"\"\n",
        "    Builds the Seq2Seq Encoder-Decoder model.\n",
        "    \"\"\"\n",
        "    # --- ENCODER ---\n",
        "    encoder_inputs = Input(shape=(max_encoder_seq_length,), name=\"encoder_input\")\n",
        "    # Embedding layer\n",
        "    enc_emb = Embedding(input_vocab_size, latent_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "    # LSTM layer\n",
        "    # We discard encoder outputs and only keep the final states (h and c).\n",
        "    encoder_lstm = LSTM(latent_dim, return_state=True, name=\"encoder_lstm\")\n",
        "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
        "    # These states will be used as the initial state for the decoder.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # --- DECODER ---\n",
        "    decoder_inputs = Input(shape=(max_decoder_seq_length,), name=\"decoder_input\")\n",
        "    # Embedding layer\n",
        "    # Note: The decoder embedding size must match the encoder's latent_dim if you share them,\n",
        "    # but here we use the same latent_dim for simplicity.\n",
        "    dec_emb_layer = Embedding(target_vocab_size, latent_dim, name=\"decoder_embedding\")\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "    # Decoder LSTM\n",
        "    # We set this LSTM to return the full sequence of outputs.\n",
        "    # It uses the encoder's states as its initial state.\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "    # Output layer\n",
        "    # A Dense layer with a softmax activation to get a probability distribution\n",
        "    # over the target vocabulary for each time step.\n",
        "    decoder_dense = Dense(target_vocab_size, activation=\"softmax\", name=\"decoder_output\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # --- MODEL ---\n",
        "    # The model that turns `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Create dummy files (comment out if using your real files)\n",
        "    create_dummy_files()\n",
        "\n",
        "    # 2. Define filepaths\n",
        "    english_file = \"/content/dataset/Dataset/english-corpus.txt\"\n",
        "    urdu_file = \"/content/dataset/Dataset/urdu-corpus.txt\"\n",
        "\n",
        "    # 3. Load and preprocess data\n",
        "    input_texts, target_texts = load_data(english_file, urdu_file, NUM_SAMPLES)\n",
        "\n",
        "    if input_texts:\n",
        "        # 4. Build tokenizers\n",
        "        input_tokenizer, target_tokenizer = build_tokenizers(input_texts, target_texts)\n",
        "\n",
        "        # 5. Prepare sequences\n",
        "        encoder_input_data, decoder_input_data, max_enc_len, max_dec_len = prepare_sequences(\n",
        "            input_texts, target_texts, input_tokenizer, target_tokenizer\n",
        "        )\n",
        "\n",
        "        # 6. Prepare decoder target data\n",
        "        # This is the \"teacher forcing\" part. The decoder_target_data is one\n",
        "        # time-step ahead of the decoder_input_data.\n",
        "        # e.g., input = \"[START] how are you\"\n",
        "        #       target = \"how are you [END]\"\n",
        "        decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "        for i, seq in enumerate(decoder_input_data):\n",
        "            decoder_target_data[i, :-1] = seq[1:]\n",
        "        # Add a third dimension for the loss function\n",
        "        decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "        # Get vocabulary sizes (add 1 for the 0-padding)\n",
        "        input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "        target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "        print(f\"Input vocabulary size: {input_vocab_size}\")\n",
        "        print(f\"Target vocabulary size: {target_vocab_size}\")\n",
        "\n",
        "        # 7. Build the model\n",
        "        model = build_model(\n",
        "            input_vocab_size, target_vocab_size, max_enc_len, max_dec_len, LATENT_DIM\n",
        "        )\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        # 8. Train the model\n",
        "        print(\"\\n--- Starting Model Training ---\")\n",
        "        model.fit(\n",
        "            [encoder_input_data, decoder_input_data],\n",
        "            decoder_target_data,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=EPOCHS,\n",
        "            validation_split=0.2,\n",
        "        )\n",
        "        print(\"--- Model Training Complete ---\")\n",
        "\n",
        "        # Note: Saving the model and creating a separate \"inference\" model\n",
        "        # (to translate new sentences) is the next step.\n",
        "        # This script only covers the training.\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T04:19:58.136014Z",
          "iopub.execute_input": "2025-10-30T04:19:58.136348Z",
          "iopub.status.idle": "2025-10-30T04:20:37.930221Z",
          "shell.execute_reply.started": "2025-10-30T04:19:58.136323Z",
          "shell.execute_reply": "2025-10-30T04:20:37.928293Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yvq1lxNW1sgM",
        "outputId": "a60d0234-08ac-4a67-ad89-cd2fe4f070f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dummy files (english-corpus.txt, urdu-corpus.txt)...\n",
            "Dummy files created successfully.\n",
            "\n",
            "Loaded 20000 sentence pairs.\n",
            "Max encoder sequence length: 16\n",
            "Max decoder sequence length: 21\n",
            "Input vocabulary size: 5146\n",
            "Target vocabulary size: 5398\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,317,376\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,381,888\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m525,312\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m5398\u001b[0m)  │  \u001b[38;5;34m1,387,286\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,317,376</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,381,888</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ encoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5398</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,387,286</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,137,174\u001b[0m (19.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,137,174</span> (19.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,137,174\u001b[0m (19.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,137,174</span> (19.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training ---\n",
            "Epoch 1/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 1s/step - accuracy: 0.7160 - loss: 2.6904 - val_accuracy: 0.7776 - val_loss: 1.3849\n",
            "Epoch 2/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 1s/step - accuracy: 0.7798 - loss: 1.3301 - val_accuracy: 0.7883 - val_loss: 1.2985\n",
            "Epoch 3/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 1s/step - accuracy: 0.7933 - loss: 1.2401 - val_accuracy: 0.7996 - val_loss: 1.2190\n",
            "Epoch 4/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 1s/step - accuracy: 0.8077 - loss: 1.1407 - val_accuracy: 0.8125 - val_loss: 1.1375\n",
            "Epoch 5/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 1s/step - accuracy: 0.8218 - loss: 1.0455 - val_accuracy: 0.8242 - val_loss: 1.0587\n",
            "Epoch 6/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 1s/step - accuracy: 0.8345 - loss: 0.9480 - val_accuracy: 0.8314 - val_loss: 0.9932\n",
            "Epoch 7/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 1s/step - accuracy: 0.8444 - loss: 0.8687 - val_accuracy: 0.8418 - val_loss: 0.9412\n",
            "Epoch 8/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 1s/step - accuracy: 0.8557 - loss: 0.7945 - val_accuracy: 0.8486 - val_loss: 0.8957\n",
            "Epoch 9/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 1s/step - accuracy: 0.8663 - loss: 0.7148 - val_accuracy: 0.8520 - val_loss: 0.8651\n",
            "Epoch 10/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 1s/step - accuracy: 0.8727 - loss: 0.6598 - val_accuracy: 0.8561 - val_loss: 0.8353\n",
            "Epoch 11/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 1s/step - accuracy: 0.8814 - loss: 0.5993 - val_accuracy: 0.8590 - val_loss: 0.8167\n",
            "Epoch 12/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 1s/step - accuracy: 0.8873 - loss: 0.5497 - val_accuracy: 0.8626 - val_loss: 0.7978\n",
            "Epoch 13/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 1s/step - accuracy: 0.8959 - loss: 0.4967 - val_accuracy: 0.8658 - val_loss: 0.7802\n",
            "Epoch 14/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 1s/step - accuracy: 0.9020 - loss: 0.4538 - val_accuracy: 0.8680 - val_loss: 0.7682\n",
            "Epoch 15/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - accuracy: 0.9116 - loss: 0.4073 - val_accuracy: 0.8712 - val_loss: 0.7539\n",
            "Epoch 16/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1s/step - accuracy: 0.9189 - loss: 0.3660 - val_accuracy: 0.8738 - val_loss: 0.7501\n",
            "Epoch 17/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 1s/step - accuracy: 0.9263 - loss: 0.3336 - val_accuracy: 0.8754 - val_loss: 0.7456\n",
            "Epoch 18/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 1s/step - accuracy: 0.9350 - loss: 0.2942 - val_accuracy: 0.8766 - val_loss: 0.7398\n",
            "Epoch 19/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 1s/step - accuracy: 0.9415 - loss: 0.2652 - val_accuracy: 0.8776 - val_loss: 0.7396\n",
            "Epoch 20/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 1s/step - accuracy: 0.9484 - loss: 0.2375 - val_accuracy: 0.8801 - val_loss: 0.7338\n",
            "--- Model Training Complete ---\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "#\n",
        "#                         INFERENCE SETUP\n",
        "#             (Run this in a new cell after training)\n",
        "#\n",
        "# =================================================================\n",
        "\n",
        "# --- 1. Re-build the Encoder Model ---\n",
        "# This model takes the English sentence and outputs the LSTM states (the \"thought vector\")\n",
        "encoder_inputs = model.input[0]  # This is the encoder_inputs from training\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.get_layer('encoder_lstm').output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# --- 2. Re-build the Decoder Model ---\n",
        "# This model takes the [START] token (and subsequent tokens) + the encoder's states\n",
        "# and outputs the next predicted word + its own new states.\n",
        "\n",
        "# Define new Input layers for the decoder's states\n",
        "decoder_state_input_h = Input(shape=(LATENT_DIM,), name='decoder_state_input_h')\n",
        "decoder_state_input_c = Input(shape=(LATENT_DIM,), name='decoder_state_input_c')\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Input for a single token (at each step of the loop)\n",
        "decoder_input_single = Input(shape=(1,), name='decoder_input_single')\n",
        "\n",
        "# Get the trained layers\n",
        "dec_emb_layer = model.get_layer('decoder_embedding')\n",
        "decoder_lstm = model.get_layer('decoder_lstm')\n",
        "decoder_dense = model.get_layer('decoder_output')\n",
        "\n",
        "# Wire the layers for inference\n",
        "dec_emb_single = dec_emb_layer(decoder_input_single)\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    dec_emb_single, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# The final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_input_single] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "print(\"Inference models (encoder/decoder) built successfully.\")\n",
        "# decoder_model.summary() # You can uncomment this if you want to see the summary again\n",
        "\n",
        "# --- 3. Create Reverse Token Lookups ---\n",
        "# We need to map token indices (like '5') back to words (like 'hello')\n",
        "reverse_input_word_index = {i: word for word, i in input_tokenizer.word_index.items()}\n",
        "reverse_target_word_index = {i: word for word, i in target_tokenizer.word_index.items()}\n",
        "\n",
        "# Get the special [START] and [END] token indices for the target language (Urdu)\n",
        "start_token_index = target_tokenizer.word_index['[start]']\n",
        "end_token_index = target_tokenizer.word_index['[end]']\n",
        "\n",
        "\n",
        "# --- 4. Define the Translation Function ---\n",
        "\n",
        "def translate_sentence(input_sentence):\n",
        "    \"\"\"\n",
        "    Translates a single English sentence to Urdu using the inference models.\n",
        "    \"\"\"\n",
        "    # 1. Preprocess the input sentence\n",
        "    # Note: The preprocess_sentence function must be available from the training cell\n",
        "    clean_sentence = preprocess_sentence(input_sentence)\n",
        "\n",
        "    # 2. Convert to sequence and pad\n",
        "    input_seq = input_tokenizer.texts_to_sequences([clean_sentence])\n",
        "    input_seq_padded = pad_sequences(input_seq, maxlen=max_enc_len, padding='post')\n",
        "\n",
        "    # 3. Get the \"thought vector\" (initial states) from the encoder\n",
        "    states_value = encoder_model.predict(input_seq_padded, verbose=0)\n",
        "\n",
        "    # 4. Start the decoding loop\n",
        "    # Initialize the loop with the [START] token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token_index\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "\n",
        "    while not stop_condition:\n",
        "        # 5. Predict the next word\n",
        "        # Pass the current token and the current states to the decoder\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # 6. Get the most likely word (token ID)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # 7. Convert the ID to a word\n",
        "        # Use .get() for safety in case the index is 0 (padding)\n",
        "        sampled_word = reverse_target_word_index.get(sampled_token_index, '[UNK]')\n",
        "\n",
        "        # 8. Check for stop condition\n",
        "        if (sampled_word == '[end]' or\n",
        "            len(decoded_sentence) > max_dec_len):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence.append(sampled_word)\n",
        "\n",
        "        # 9. Update for the next loop\n",
        "        # The next input token is the one we just predicted\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        # The next states are the ones we just got from the decoder\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # 10. Return the final sentence\n",
        "    return \" \".join(decoded_sentence)\n",
        "\n",
        "# --- 5. Test the Translator ---\n",
        "\n",
        "print(\"\\n--- Testing Translations ---\")\n",
        "\n",
        "test_sentences = [\n",
        "    # --- Sentences from your training data ---\n",
        "    \"How are you?\",\n",
        "    \"Run.\",\n",
        "    \"This is my house.\",\n",
        "    \"I love machine learning.\",\n",
        "    \"Where is the book?\",\n",
        "    \"Hello world.\",\n",
        "    \"What is your name?\",\n",
        "    \"Python is a great language.\",\n",
        "    \"See you later.\",\n",
        "    \"This is a parallel corpus.\",\n",
        "\n",
        "    # --- New simple sentences (high chance of working) ---\n",
        "    \"This is a book.\",\n",
        "    \"You are great.\",\n",
        "    \"What is this?\",\n",
        "\n",
        "    # --- Sentences with new words (will test the [UNK] token) ---\n",
        "    \"I am learning Python.\",\n",
        "    \"She is a good person.\",\n",
        "    \"Where is the car?\",\n",
        "    \"He likes to read.\"\n",
        "]\n",
        "\n",
        "# --- 5. Test the Translator ---\n",
        "print(\"\\n--- Testing Translations ---\")\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence(sentence)\n",
        "    print(f\"Input:       {sentence}\")\n",
        "    print(f\"Translation: {translation}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T06:31:40.805980Z",
          "iopub.execute_input": "2025-10-30T06:31:40.806284Z",
          "iopub.status.idle": "2025-10-30T06:33:28.105629Z",
          "shell.execute_reply.started": "2025-10-30T06:31:40.806259Z",
          "shell.execute_reply": "2025-10-30T06:33:28.104565Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC5JkIy91sgR",
        "outputId": "8d96fdf8-364e-4fb1-cd73-c69c17d1115f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference models (encoder/decoder) built successfully.\n",
            "\n",
            "--- Testing Translations ---\n",
            "\n",
            "--- Testing Translations ---\n",
            "Input:       How are you?\n",
            "Translation: آپ کیسا ہیں\n",
            "\n",
            "Input:       Run.\n",
            "Translation: زان تخیل کی\n",
            "\n",
            "Input:       This is my house.\n",
            "Translation: یہ میرا گھر ہے\n",
            "\n",
            "Input:       I love machine learning.\n",
            "Translation: میں کتوں سے ڈرتا ہوں\n",
            "\n",
            "Input:       Where is the book?\n",
            "Translation: کتاب کہاں ہے\n",
            "\n",
            "Input:       Hello world.\n",
            "Translation: خدا نے ناشتہ بنایا\n",
            "\n",
            "Input:       What is your name?\n",
            "Translation: آپ کا نام کیا ہے\n",
            "\n",
            "Input:       Python is a great language.\n",
            "Translation: یہ بہت اچھا ہے\n",
            "\n",
            "Input:       See you later.\n",
            "Translation: بعد میں آپ کو فون کریں\n",
            "\n",
            "Input:       This is a parallel corpus.\n",
            "Translation: یہ ایک کار ہے\n",
            "\n",
            "Input:       This is a book.\n",
            "Translation: یہ ایک کتاب ہے\n",
            "\n",
            "Input:       You are great.\n",
            "Translation: آپ بہت اچھا ہیں\n",
            "\n",
            "Input:       What is this?\n",
            "Translation: یہ کیا ہے؟\n",
            "\n",
            "Input:       I am learning Python.\n",
            "Translation: میں کوریائی ہوں\n",
            "\n",
            "Input:       She is a good person.\n",
            "Translation: وہ ایک اچھا لڑکا ہے\n",
            "\n",
            "Input:       Where is the car?\n",
            "Translation: کار کہاں ہے\n",
            "\n",
            "Input:       He likes to read.\n",
            "Translation: وہ انگریزی سکھاتا کرتا ہے\n",
            "\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T06:36:16.734074Z",
          "iopub.execute_input": "2025-10-30T06:36:16.735470Z"
        },
        "id": "ldWDvDYa1sgS"
      },
      "outputs": [],
      "execution_count": 18
    }
  ]
}